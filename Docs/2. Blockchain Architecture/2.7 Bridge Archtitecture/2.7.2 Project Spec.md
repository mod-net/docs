perfect — here’s a tight, end‑to‑end development plan starting from a Substrate solo‑chain template and marching through:

* update to current libs
* wire in the existing Substrate pallet infrastructure
* add the existing IPFS module pallet

I’m assuming Polkadot SDK (Substrate) latest stable, Rust stable, and a clean repo split between `node/`, `runtime/`, and `pallets/`.

---

# Development Plan (Solochain → Bridge/Vesting/Module+IPFS ready)

## Phase 0 — Bootstrap & Upgrades

**Goals:** fresh solo‑chain, current toolchain, green build, CI.

1. **Scaffold**

* Start from `substrate-node-template` (or the “polkadot-sdk node template”).
* Rename crates: `mod-node`, `mod-runtime`, org/local path updates.

2. **Toolchain**

* Install/lock:

  * `rustup toolchain install stable` and `nightly` (for `wasm32-unknown-unknown` if needed).
  * `rustup target add wasm32-unknown-unknown --toolchain nightly`
  * `wasm-opt` via binaryen.
* Pin versions in `rust-toolchain.toml`.

3. **SDK Bump**

* Update to the latest Polkadot SDK release (crate versions must match).
* Align `Cargo.toml` features (`std`/`runtime-benchmarks`/`try-runtime`) across workspace.
* Build matrix: native + wasm (`cargo build -p mod-node`; `cargo build -p mod-runtime --target wasm32-unknown-unknown`).

4. **Housekeeping**

* Add `pre-commit` hooks (fmt, clippy, audit).
* CI: GitHub Actions with jobs for:

  * check → fmt/clippy
  * build → native + wasm
  * test → unit/integration
  * audit → `cargo-deny`, `cargo-audit`

**Exit criteria**

* Node starts (`--dev`) and produces blocks.
* CI green on all jobs.

---

## Phase 1 — Core FRAME Pallets (existing infra baseline)

**Goals:** minimal but complete runtime with balances/treasury/governance/utility/vesting.

1. **Runtime wiring**

* Add and configure:

  * `frame-system`
  * `pallet-timestamp`
  * `pallet-balances`
  * `pallet-transaction-payment`
  * `pallet-treasury`
  * `pallet-utility` (batch, multisig)
  * `pallet-collective` (Council or Technical Committee, if desired)
  * `pallet-referenda` (or `democracy`, depending on preference)
  * `pallet-vesting` (native, to start; custom unlock curve later if needed)
  * (Optional) `pallet-assets` (multi‑asset needs) and `pallet-identity`

2. **Genesis config**

* Set existential deposit (ED), initial endowed accounts, initial treasury account, initial governance bodies (or temporary `sudo` during bootstrap only; remove later).

3. **Fees & Weights**

* Calibrate `WeightToFee` and `TxByteFee`.
* Add benchmarking features and generate weight files for the included pallets.

4. **Try‑Runtime (optional but recommended)**

* Enable `try-runtime` feature for dry runs of storage migrations later.

**Exit criteria**

* Runtime compiles to WASM with all core pallets.
* Unit tests for fee/weight hooks.
* ChainSpec includes genesis balances and treasury; node produces blocks with pallets callable.

---

## Phase 2 — Existing Substrate “pallet infrastructure” (yours)

**Goals:** migrate and wire *your* existing pallets (module registry, other shared infra you already have).

1. **Pull in your pallets**

* Add your existing pallets as workspace members under `pallets/`.
* Resolve version/feature mismatches with current SDK.
* Gate optional features (e.g., benchmarking, offchain workers).

2. **Runtime integration**

* Implement each pallet’s `Config` with types/params.
* Insert into `construct_runtime!`.
* Add storage migration guards if any layout changed since last use.
* Benchmark → regenerate weights.

3. **Genesis & permissions**

* Seed any required initial storage (admin roles, registrars/owners).
* If pallets have call filters/origins, fold them into governance (Root, Council, Whitelist Origin, etc.).

**Exit criteria**

* All existing pallets compile and execute basic happy‑path calls on a local dev chain.
* Benchmarks generated; tests green.

---

## Phase 3 — Module Registry + IPFS Integration (existing IPFS module pallet)

**Goals:** register modules to IPFS, CID validation, and (optionally) pinning signals.

1. **Module Registry pallet**

* Wire your `pallet-module-registry`:

  * Storage: module ID → metadata (CIDv1, tags, owner, size limits).
  * Calls: register/update/retire/transfer.
  * Events and errors: ensure crisp, indexed.

2. **CID validation**

* Enforce CIDv1, size bounds, and deterministic hashing. Consider an on‑chain check that CID decodes and multicodec matches expectations.

3. **Pinning (optional but useful)**

* If you have/need a dedicated `pallet-ipfs-pin`:

  * Emit events `PinRequested(CID)`, `UnpinRequested(CID)`.
  * Off‑chain *infra* (not OCW) watches events and talks to your IPFS Cluster/daemon.

4. **Dev tooling**

* Simple CLI (or script) to publish metadata to IPFS and call `register`.
* Local IPFS node docs (`ipfs daemon`) and .env for API multiaddrs.

**Exit criteria**

* End‑to‑end: put metadata on IPFS → call `register` → see on‑chain state; (optionally) off‑chain service pins it.

---

## Phase 4 — Bridge & Vesting Enablement (structure in place; implementation follows)

> You said we’re starting from requirements and infra — this phase sets the scaffolding so implementing claims/vesting math later is painless.

1. **Bridge pallet skeleton (`pallet-bridge`)**

* Storage (placeholders first): `MerkleRoot`, `SnapshotBlock`, `BaseRatio`, `Params { TMin, TMax, K, UnlockShape }`, `Claimed`.
* Calls: `set_params(...)`, `pause()/unpause()` (guarded), **stub** `claim(...)` (todo: verify merkle proof & mint vesting).
* Events: `ParamsUpdated`, `Paused/Unpaused`, `Claimed(...)` (stub emit for e2e later).
* Hooks for pausing & admin changes under governance origin.

2. **Vesting strategy**

* For MVP wire native `pallet-vesting` (linear).
* If you need convex/back‑loaded unlock, either:

  * extend with an auxiliary pallet managing custom schedules, or
  * fork `pallet-vesting` with an unlock‑curve parameter.

3. **Snapshot + Merkle tooling (off‑chain repo)**

* CLI to:

  * import balances at snapshot height,
  * filter dust < ED,
  * build canonical JSON,
  * construct Merkle tree (leaf = `(address, balance, snapshot_block, chainId, salt)`),
  * output `root` and proofs, publish snapshot JSON to IPFS.

4. **Relayer skeleton**

* A tiny service that:

  * tails Base L2 claim events (from the simple EVM claims contract),
  * submits `claim(proof, leaf, T_days)` to our chain,
  * idempotency & replay protection (client‑side and pallet‑side),
  * rate limiting & retries.

**Exit criteria**

* Bridge pallet compiles with governance‑updatable params and pause switch.
* Tooling repos stubbed with README + command skeletons.
* Local e2e “dry run” path documented (without final proof verification).

---

## Phase 5 — Observability & DX

**Goals:** make the thing pleasant to ship and debug.

* Prometheus metrics and logs for node.
* Indexer (Subsquid or SubQuery) + dashboard panels:

  * `Claimed` events, chosen `T` distribution, treasury vesting trajectory.
* Dev UX:

  * `make devnet` target spins a local 2‑node network with deterministic keys.
  * `make upgrade` shows try‑runtime dry run of storage changes.

**Exit criteria**

* Dashboard shows basic chain activity; local dev scripts reproducible.

---

## Phase 6 — Security Hardening

**Goals:** pause switch sanity, replay protection, fuzzing.

* Unit/property tests:

  * pause gates,
  * parameter changes via governance only,
  * (later) merkle proof verification matrix,
  * vesting math invariants (sum of schedules ≤ supply).
* Fuzz:

  * claim replay/double‑spend attempts,
  * malformed leaves/proofs,
  * boundary T (min/max) and multiplier edge cases.
* Supply checks:

  * totals equal snapshot + treasury allocation (unclaimed path).

**Exit criteria**

* Test suite covers critical paths; invariant checks pass.

---

## Deliverables & Milestones (sequenced)

* **M0** — Solochain template upgraded, CI green, devnet script.
* **M1** — Core FRAME pallets wired (balances/treasury/governance/utility/vesting). Benchmarks done.
* **M2** — Your existing pallets integrated (module registry et al.), weights regenerated.
* **M3** — IPFS module pallet live, optional pinning flow with off‑chain agent.
* **M4** — Bridge pallet skeleton + params/pause, snapshot/Merkle/relayer skeletons.
* **M5** — Observability (indexer + dashboard), hardening pass.

---

## Concrete TODOs (first three bullets you asked for)

1. **Update template to current libraries**

* [ ] Replace node template with latest Polkadot SDK template.
* [ ] Pin toolchains; align crate versions; compile native + wasm.
* [ ] Add CI (fmt/clippy/build/test/audit) and pre‑commit hooks.

2. **Add all existing Substrate pallet infrastructure**

* [ ] Add FRAME core pallets listed in Phase 1; implement `Config` blocks.
* [ ] Add your existing pallets; resolve feature/version drift; run benchmarks.
* [ ] Extend ChainSpec for genesis roles/balances.
* [ ] Add unit tests for each pallet origin/call surface.

3. **Add existing IPFS module pallet**

* [ ] Wire `pallet-module-registry` into runtime; expose extrinsics.
* [ ] Validate CIDv1 and size bounds in the call path.
* [ ] (Optional) Add `pallet-ipfs-pin` and emit pin/unpin events.
* [ ] Provide CLI scripts for `register` and a local IPFS setup doc.

---

## Notes & Options

* **Sudo vs Governance:** keep `pallet-sudo` only for early bootstrap; remove once `referenda/collective` can manage params.
* **Custom Vesting Curve:** start linear, then add convex/back‑loaded curve once bridge math is finalized.
* **EVM on our chain:** not required for Base‑anchored claims; only add Frontier if you want EVM contracts on *our* chain too.
* **Migration safety:** enable `try-runtime` before any on‑chain upgrade that touches storage layouts.

---

If you want, I can turn this into a repo checklist (`/docs/dev-plan.md`) plus an initial `ChainSpec` with stubbed accounts so you can start committing against it right away.
